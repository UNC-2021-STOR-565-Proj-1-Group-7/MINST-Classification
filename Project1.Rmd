---
title: "{Fill in title}"
author: "Shilong Dai, <Add Your Name>"
header-includes:
- \usepackage{amsgen,amsmath,amstext,amsbsy,amsopn,amssymb,mathabx,amsthm,bm,bbm}
- \usepackage[labelsep=space]{caption}
output:
  pdf_document: default
  word_document: default
  html_document: default
subtitle: $\textbf{Machine Learning, Spring 2021}$
---

# Abstract
*{Fill in your abstract}*

# Introduction
*{Fill in your introduction}*

# Setup
```{r}
set.seed(315)
# Load necessary packages
library(dslabs)
library(ggplot2)
```


# Loading and Preprocessing the Data
1. Begin by loading the dataset you've chosen. Depending on which dataset you've chosen, it may be helpful to consider a subset of the data for your analyses. Give an explanation as to why you did or did not subset your data.
```{r}
minst_data <- read_mnist()
minst.train_raw <- minst_data$train
minst.test_raw <- minst_data$test

```

2. Next, provide some basic information about the data. What are the samples and what are the variables? How many samples are there? How many variables are there? How many of those variables are numerical vs categorical? How many of the numerical variables are discrete? Discuss any other information about the data that you think is relevant.
```{r}
print("Training Sample Size")
length(minst.train_raw$labels)
print("Training Image Size")
length(minst.train_raw$images[1, ])
print("Training Number 1")
minst.train_raw$labels[1]
print("Training Number Range")
sort(unique(minst.train_raw$labels))
print("Test Sample Size")
length(minst.test_raw$labels)
print("Test Image Size")
length(minst.test_raw$images[1, ])
print("Test Number 1")
minst.test_raw$labels[1]
print("Test Number Range")
sort(unique(minst.test_raw$labels))
```

The data consists of a set of training and test samples. For both the training and test samples, the sample consists of a sequence of images in greyscale and labels. Each image is flattened from 2D to a 1D sequence of values between 0 and 255, which indicates the blackness of the pixel. After flattening, there are 784  Each label is a number corresponding to the actual handwritting digit of the associated image.

There are a total of 60000 training samples, and 10000 test samples. Since all representations are either digital or labels indicating the written digit, both the greyscale value and the digit label are discrete. Each greyscale value corresponds to an pixel and thus is natural numbers on [0, 255]. The label of the numbers themselves are categorical variables between 0 and 9, representing the nine digits.

3. In some scenarios, it may be helpful to transform or preprocess your data in some way before embarking on your analysis. For example, one may wish to center and standardize each numerical variable before performing certain regression analyses. On the other hand, one may wish to group some factors of a categorical variable together. Perform any preprocessing that you feel is appropriate and discuss the motivation for this choice. If you decide not to, explain why. Note that it may be helpful to omit this step and return at a later point once you have explored the data a bit.
```{r}
preprocess_minst <- function (images, labels) {
  image_col_names = c("digit")
  for (i in 1:length(images[1, ])) {
    image_col_names = c(image_col_names, sprintf("p_%d", i))
  }

  result <- as.data.frame(cbind(labels, images))
  colnames(result) <- image_col_names
  return(result)
}

split_minst <- function (minst, digit, train_amount = 2000, test_amount = 1000) {
  selected_digit <- minst[minst[["digit"]] == digit, ]
  train_rows <- sample(nrow(selected_digit), train_amount)
  train_sample <- minst[train_rows, ]
  remainder <- minst[-train_rows, ]
  test_rows <- sample(nrow(remainder), test_amount)
  test_sample <- remainder[test_rows, ]
  return(list(train = train_sample, test = test_sample))
}

minst.train <- preprocess_minst(minst.train_raw$images, minst.train_raw$labels)
minst.test <- preprocess_minst(minst.test_raw$images, minst.test_raw$labels)
minst.combined <- rbind(minst.train, minst.test)

minst_final.test <- data.frame()
minst_final.train <- data.frame()
for(i in 0:9) {
  sampled <- split_minst(minst.combined, i)
  minst_final.test <- rbind(minst_final.test, sampled$test)
  minst_final.train <- rbind(minst_final.train, sampled$train)
}
minst_final.together <- rbind(minst_final.train, minst_final.test)


head(minst_final.train)
head(minst_final.test)
```

The images and labels are combined and matched together into the sample and test dataframes. The rows in each dataframe represents the pixels of an image with its label. This is done in order to have the image and the label variable together for easier processing.

TODO: Combined test and train, sampled 2000 per digit for train and 1000 per digit for test etc.

# Exploratory Data Analysis
4. Provide a statistical summary of the data as well as a summary of the data in words. Note that it is not necessary to list every summary statistic for every variable. Rather, try to give the reader a sense of what kinds of values the variables take on. And don't forget any categorical variables!
```{r, eval=FALSE}

Mode <- function(x) {
  ux <- unique(x)
  return(ux[which.max(tabulate(match(x, ux)))])
}

image_only_minst <- minst_final.together[, -c(1)]
flattened_images <- as.vector(t(image_only_minst))

center_pixels <- as.vector(t(minst_final.together["p_392"]))
edge_pixels <- as.vector(t(minst_final.together['p_1']))
rand_pixels <- as.vector(t(minst_final.together['p_457']))

flattened_sum <- summary(flattened_images)
center_sum <- summary(center_pixels)
corner_sum <- summary(edge_pixels)
rand_sum <- summary(rand_pixels)

sd_flattened <- sd(flattened_images)
sd_center <- sd(center_pixels)
sd_corner <- sd(edge_pixels)
sd_rand <- sd(rand_pixels)
sd_sum <- c(sd_flattened, sd_center, sd_corner, sd_rand)

mode_flattened <- Mode(flattened_images)
mode_center <- Mode(center_pixels)
mode_corner <- Mode(edge_pixels)
mode_rand <- Mode(rand_pixels)
mode_sum <- c(mode_flattened, mode_center, mode_corner, mode_rand)

sum_table <- rbind(flattened_sum, center_sum, corner_sum, rand_sum)
sum_table <- cbind(sum_table, sd_sum, mode_sum)

colnames(sum_table) <- c(colnames(sum_table)[1:6], "StDev", "Mode")
rownames(sum_table) <- c("All", "Center", "Corner", "P. 457")
print(sum_table)
```

5. Pick three variables in your dataset and visualize the distribution of each in separate plots. Remember to properly title and label each plot. Compare them and comment on what you see. Do any of the distributions appear similar or different? In what ways?
```{r, eval=FALSE}
ggplot(data = minst_final.together, aes(x = p_457)) + geom_histogram()
ggplot(data = minst_final.together, aes(x = p_1)) + geom_histogram(binwidth = 1)
ggplot(data = minst_final.together, aes(x = p_536)) + geom_histogram()
```


6. Pick two numerical variables in your dataset. What is their correlation? Visualize their joint behavior in a single plot. Does this suggest that there is a strong relationship between the two?
```{r, eval=FALSE}
# Compute correlation.
# Plot joint behavior.
```

7. Pick two numerical variables in your dataset. Perform a hypothesis test to check whether their means are equal. Discuss what test you performed and why. What is the result of the test? Note that such a test won't make sense if you have centered your data so you should apply this to your data without any centering.
```{r, eval=FALSE}
# Perform hypothesis test.
```

8. In a single plot, visualize the relationship between the means or medians and the standard deviations of each variable. Do you notice any outliers? 
```{r, eval=FALSE}
# Plot means or medians vs sds.
```


# Principal Components Analysis
9. Next, we would like to reduce the dimension of the data. Explain what the "dimension" of a dataset is. Discuss why one would want to reduce it. Provide an intuitive explanation of how PCA can achieve this.

10. Run PCA on the dataset. Decide whether to set *scale = TRUE* or *scale = FALSE* and explain your choice. Provide a numerical summary of the first 5 PCs. Note that you should leave any categorical variables out of this analysis.
```{r, eval=FALSE}
# Run PCA.
```

11. Plot a screeplot of the PCs. How many principal components are required to explain at least 80% of the variation in the data? Based on this and the plot, does it seem like PCA has done a good job in reducing the dimensionality of the data?
```{r, eval=FALSE}
# Plot screeplot.
```

12. Visualize the distributions of the first 3 PCs in separate plots and comment. Are there any apparent clusters? Do any of the first 3 PCs appear most helpful for separating the data?
```{r, eval = FALSE}
# Plot first 3 PCs.
```


13. Plot the first 3 PCs against one another (see the Biplots section of Computing Assignment 3). Are there any apparent clusters?
```{r, eval = FALSE}
# Plot first 3 PCs against one another.
```

# Clustering
14. Next we would like to perform a cluster analysis on the data. Explain intuitively what that means. What exactly is a cluster? What are the objects being clustered? Why might this be helpful for this dataset?

15. Apply one of the clustering algorithms you learned about in class to your data. Depending on the algorithm you use, you may need to specify a cutpoint to obtain a single cluster label for each sample. Note also that depending on the dimension of your dataset, it may be preferable to apply the clustering algorithm to the first few PCs of your data rather than the data itself. Elaborate upon your choice.
```{r, eval = FALSE}
# Apply the clustering algorithm.
```

16. What did the clustering algorithm find? Are the clusters relatively homogeneous or heterogeneous? Summarize the results.
```{r, eval = FALSE}
# Summarize results of clustering.
```

17. What is the within-cluster sum of squares for the clusters you found? How does this compare to the total sum of squares?
```{r, eval = FALSE}
# Compute sums of squares.
```

18. Plot the first 3 PCs against one another again but include the cluster label for each point. Do any patterns emerge? Comment on what you see.
```{r, eval = FALSE}
# Plot PCs with clusters.
```

# Classification
19. Next we would like to perform classification on the data. Explain intuitively what that means. What does it mean to classify the data? What are the objects being classified? Why might this be of interest for this dataset?

20. Identify a categorical variable in the data to correspond to the class of each sample in the dataset. In particular, is there a variable that might be interesting to predict from the other variables?

21. Construct a table that describes how many samples in each class fall into each cluster. How well was your previous cluster analysis able to identify the classes you chose? Are any of the clusters comprised mostly by one class?
```{r, eval=FALSE}
# Construct table.
```

22. Randomly break your dataset into a training set (roughly 80% of the data) and a test set (roughly 20% of the data). What is the point of doing this before doing classification?
```{r, eval = FALSE}
# Break into train and test sets.
```

23. Using the class labels based on the categorical variable you chose earlier, apply a classification method to the training set. Be sure to specify any free parameters that were chosen. What is the accuracy of the fitted model on the training set?
```{r, eval = FALSE}
# Run classification algorithm.
# Compute training set accuracy.
```

24. What is the accuracy of the fitted model on the test set? How does this compare to the accuracy on the training set? Does this make sense? Explain why or why not.
```{r, eval = FALSE}
# Compute accuracy on test set.
```

25. Construct a confusion matrix based on the results of classification. Were there any classes on which your algorithm performed better than others? What about worse than others? Does this make sense based on the nature of the data?
```{r, eval = FALSE}
# Find classes where the algorithm performed best / worst.
```


# Conclusion
*{Fill in your conclusion}*

