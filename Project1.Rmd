---
title: "MINST handwritten digits report"
author: "Shilong Dai, Alex Mangiapane, Yinuo Hu"
header-includes:
- \usepackage{amsgen,amsmath,amstext,amsbsy,amsopn,amssymb,mathabx,amsthm,bm,bbm}
- \usepackage[labelsep=space]{caption}
output:
  pdf_document: default
  word_document: default
  html_document: default
subtitle: $\textbf{Machine Learning, Spring 2021}$
---

# Abstract
Our report studied the MNIST handwritten digits data by conducting exploratory data analysis, principal component analysis, unsupervised K-means clustering method, and supervised K-Nearest Neighbor classification algorithm. First, We provided statistical summary of the MNIST handwritten digits data to show some of its distributions and variations. Since our data contained high dimensions, we utilized principal component analysis for dimension reduction. Then, we used unsupervised clustering algorithms to group the data into 10 clusters. With the 10 clusters, we studied the composition of the clusters in relation to the digit labels. Later, We applied KNN classification algorithm to classify the digits with 5 nearest neighbors. We found that KNN performs decently on the MNIST data set. Experimenting on the raw and dimension reduced data (43 PCs), the KNN algorithm is able to classify the training digits with accuracies between 95-96%. The model on the raw training set performed slightly worse on the testing set with an accuracy of 93.1%, while the model on the dimension reduced training set performed almost the same with an accuracy of 95.4%.

# Introduction

We used the MNIST handwritten digits dataset from http://yann.lecun.com/exdb/mnist/, which is a collection of grey-scale images of handwritten digits that contains 1 categorical variable indicating the label of digits (from 0-9) and 784 numerical variable representing the pixels of the images. Since some of the handwritten digits are curly and blur which may create difficulties for computers to classify, it is interesting to explore how well machine learning algorithms may perform for classification and clustering of these handwritten digits images. The applicability of machine learning to digit classification may be a useful endeavor in many fields, especially for postal services. An automatic identification of digit may save human resources, and also improves efficiency in jobs that involves recognizing visual, numerical inputs.
Since our dataset have relatively high dimension, we might encounter the curse of dimensionality, for which the increasing number of attributes may deteriorate the predictive power of a classifier instead of improving steadily. We will apply principal component analysis for dimension reduction and we might apply clustering method on major PCs to avoid such issue. For classification we will compare the model trained with original high-dimension data and reduced-dimension data to compare the effectiveness of our model considering the problem of dimensionality.  

# Setup
```{r warning = FALSE, message = FALSE}
set.seed(315)
# Load necessary packages
library(dslabs)
library(ggplot2)
library(GGally)
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization
library(caret)
library(class)
```


# Loading and Preprocessing the Data
1. Begin by loading the dataset you've chosen. Depending on which dataset you've chosen, it may be helpful to consider a subset of the data for your analyses. Give an explanation as to why you did or did not subset your data.

During the analysis, the source website of the data turns out to be unreliable. Therefore, we saved the dataset locally. In the local version, we matched the pixels against the labels, so that they are in the same dataframe.Therefore, in the following sections, the data would be loaded locally, instead of reading from the website each time.

Since our dataset has 70000 observations in total, and it may take long to train the model when running our later classification algorithm, so we take a subset of total 5000 observations. First, we converted the raw data to a dataframe, where the columns include the digit and the pixel values, and each row is a single image. Then, we combined the test and training set from the original MNIST collection to arrive at a single pool of images. From the combined pool, we did stratified sampling to ensure that there are an equal amount of sample for each of the digits. Since we have 10 digits in total, we randomly picked 500 images per digit, which results in a total of 5000 images overall.

```{r warning = FALSE, message = FALSE}
#minst_data <- read_mnist()
#minst.train_raw <- minst_data$train
#minst.test_raw <- minst_data$test


preprocess_minst <- function (images, labels) {
  image_col_names = c("digit")
  for (i in 1:length(images[1,])) {
    image_col_names = c(image_col_names, sprintf("p_%d", i))
  }
  
  result <- as.data.frame(cbind(labels, images))
  colnames(result) <- image_col_names
  return(result)
}

split_minst <- function (minst, digit, sample_size = 500) {
  selected_digit <- minst[minst[["digit"]] == digit,]
  sampled_rows <- sample(nrow(selected_digit), sample_size)
  
  return(selected_digit[sampled_rows,])
}

#minst.train <- preprocess_minst(minst.train_raw$images, minst.train_raw$labels)
#minst.test <- preprocess_minst(minst.test_raw$images, minst.test_raw$labels)

minst.train <- read.csv("minst_train.csv")
minst.test <- read.csv("minst_test.csv")
minst.combined <- rbind(minst.train, minst.test)

minst_sample.together <- data.frame()
for (i in 0:9) {
  sampled <- split_minst(minst.combined, i)
  minst_sample.together <- rbind(minst_sample.together, sampled)
}

```

2. Next, provide some basic information about the data. What are the samples and what are the variables? How many samples are there? How many variables are there? How many of those variables are numerical vs categorical? How many of the numerical variables are discrete? Discuss any other information about the data that you think is relevant.

The data consists of a set of black and white images with the label for the digit it represent. Each image is flattened from 2D to a 1D sequence of values row by row. Each label is a number corresponding to the actual handwritting digit of the associated image. Furthermore, the images are bounded by a box from the original handwritting images, and centered by mass, according to the source of the data.

After our initial preprocessing, each pixel is represented by the column "p_num" in the dataframe, where "num" is the index of the pixel in the flattened 1D sequence. Since there are 784 pixels, there are 784 pixel columns. Each pixel takes a discrete greyscale value between 0 and 255, where the greater the number, the darker the pixel. Furthermore, the label for the image is associated with each image via the column "digit." All 10 digits are included, so the "digit" column takes the value of integer from 0 to 9, representing the actual digit of the handwriting respectively. Therefore, there are 784 numerical variables in this dataset consisting of the pixel greyscale values, and 1 categorical variable consiting of the label of the handwriting image. As indicated before, there are 500 images associated with each handwritten digit.

```{r warning = FALSE, message = FALSE}
summary_minst_init <- data.frame(
  Pixels = length(minst_sample.together) - 1,
  Digits = length(unique(minst_sample.together[["digit"]])),
  Image.Count = nrow(minst_sample.together[minst_sample.together[["digit"]] == 1, ]),
  Max.Pixel = max(minst_sample.together[, -c(1)]),
  Min.Pixel = min(minst_sample.together[, -c(1)])
)
print(summary_minst_init)
```

3. In some scenarios, it may be helpful to transform or preprocess your data in some way before embarking on your analysis. For example, one may wish to center and standardize each numerical variable before performing certain regression analyses. On the other hand, one may wish to group some factors of a categorical variable together. Perform any preprocessing that you feel is appropriate and discuss the motivation for this choice. If you decide not to, explain why. Note that it may be helpful to omit this step and return at a later point once you have explored the data a bit.

All of the images are centered by mass, as indicated on the MNIST website, with greyscale values between 0 and 255. Therefore, no normalization is necessary on this dataset. There are 10 digits as categories for this dataset. We did not group them because each is a distinct, equal class of data by itself.

# Exploratory Data Analysis
4. Provide a statistical summary of the data as well as a summary of the data in words. Note that it is not necessary to list every summary statistic for every variable. Rather, try to give the reader a sense of what kinds of values the variables take on. And don't forget any categorical variables!

```{r warning = FALSE, message = FALSE}

Mode <- function(x) {
  ux <- unique(x)
  return(ux[which.max(tabulate(match(x, ux)))])
}

image_only_minst <- minst_sample.together[,-c(1)]
flattened_images <- as.vector(t(image_only_minst))

center_pixels <- as.vector(t(minst_sample.together["p_392"]))
edge_pixels <- as.vector(t(minst_sample.together['p_1']))
rand_pixels <- as.vector(t(minst_sample.together['p_457']))

flattened_sum <- summary(flattened_images)
center_sum <- summary(center_pixels)
corner_sum <- summary(edge_pixels)
rand_sum <- summary(rand_pixels)

sd_flattened <- sd(flattened_images)
sd_center <- sd(center_pixels)
sd_corner <- sd(edge_pixels)
sd_rand <- sd(rand_pixels)
sd_sum <- c(sd_flattened, sd_center, sd_corner, sd_rand)

mode_flattened <- Mode(flattened_images)
mode_center <- Mode(center_pixels)
mode_corner <- Mode(edge_pixels)
mode_rand <- Mode(rand_pixels)
mode_sum <- c(mode_flattened, mode_center, mode_corner, mode_rand)

sum_table <- rbind(flattened_sum, center_sum, corner_sum, rand_sum)
sum_table <- cbind(sum_table, sd_sum, mode_sum)

colnames(sum_table) <- c(colnames(sum_table)[1:6], "StDev", "Mode")
rownames(sum_table) <- c("All", "Center", "Corner", "P. 457")
print(as.data.frame(sum_table))
```

Rather than give a summary for each of the 784 pixels in each image, instead we gave a summary of four important categories: the entire set of pixel values, the center pixel (Pixel 392), a corner pixel (Pixel 1), and a random pixel (Pixel 457). It turns out that some writings do pass through the center of the image in contrary to the corner pixel, as indicated by a non-zero mean with a max value of 51. In fact, given that the corner pixel has a max of 0, no writing was in the upper left corner of any image sampled. However, it is still relatively uncommon for the digit to pass through the exact center, since the center mean is much lower than the overall mean of the greyscale values, while the standard deviation is also much lower. Our randomly chosen pixel happened to be a darker than average pixel, as clearly displayed with a mean of 67.90 darkness, and with a greater standard deviation. Also note that the mode of each of these categories is 0, which means that for a given image, most of the space on the canvas is blank.

5. Pick three variables in your dataset and visualize the distribution of each in separate plots. Remember to properly title and label each plot. Compare them and comment on what you see. Do any of the distributions appear similar or different? In what ways?
```{r warning = FALSE, message = FALSE}
ggplot(data = minst_sample.together, aes(x = p_457)) + 
  geom_histogram() + ggtitle("Distribution of Pixel 457") + 
  xlab("Pixel Values") + ylab("Frequency")
ggplot(data = minst_sample.together, aes(x = p_1)) + 
  geom_histogram(binwidth = 1) + ggtitle("Distribution of Pixel 1") + 
  xlab("Pixel Values") + ylab("Frequency")
ggplot(data = minst_sample.together, aes(x = p_537)) + 
  geom_histogram() + ggtitle("Distribution of Pixel 536") + 
  xlab("Pixel Values") + ylab("Frequency")
```
As is clear in the histograms, we can clearly see that in pixel 1, a corner pixel, every image had a value of 0. This confirms the summary statistics in the previous section. Our other two chosen pixels, Pixel 457 and Pixel 537, had more interesting distributions. Pixel 457 still had mostly 0's, but also many 255's, with some much smaller frequency values spread out in between. Pixel 537 also had mostly 0's, with a very small amount of 255's, and almost no values in-between. As such, Pixel 537 resembled Pixel 1 fairly closely, with almost entirely 0's, whereas Pixel 457 had a slightly greater range of values.

6. Pick two numerical variables in your dataset. What is their correlation? Visualize their joint behavior in a single plot. Does this suggest that there is a strong relationship between the two?
```{r warning = FALSE, message = FALSE}
cor(minst_sample.together["p_455"], minst_sample.together["p_456"])
plot(
  minst_sample.together$p_455,
  minst_sample.together$p_456,
  main = "Plot of Pixel 455 Values Vs. Pixel 456 Values",
  xlab = "Pixel 455 Values",
  ylab = "Pixel 456 Values"
)
```
Here, we arbitrarily chose to find the correlation between two pixels right next to each other (Pixels 455 and 456), expecting a reasonably strong correlation because of their proximity. We calculated the correlation between these two pixel values to be 0.779, a strong correlation. As can be seen in the plot, there are many data points clustered in the top corners of the plot, with the data becoming increasingly sparse toward the middle. Thus, the plot does suggest a reasonable amount of correlation betweene the variables. 

7. Pick two numerical variables in your dataset. Perform a hypothesis test to check whether their means are equal. Discuss what test you performed and why. What is the result of the test? Note that such a test won't make sense if you have centered your data so you should apply this to your data without any centering.
```{r}
mean(minst_sample.together$p_455)
mean(minst_sample.together$p_456)
wilcox.test(minst_sample.together$p_455, minst_sample.together$p_456)
```
We can initially see that the means of the pixel darkness in Pixel 456 and Pixel 455 are quite different, so to determine if this is statistically significant, we use wilcox.test (since the data is paired and is not normally distributed, so we cannot just use t.test). Here, our p-value is extremely close to 0, so we can confidently reject our null hypothesis and conclude that these to variables have unequal means. Note: this test is known as the Wilcoxon Signed Rank Test.

8. In a single plot, visualize the relationship between the means or medians and the standard deviations of each variable. Do you notice any outliers? 
```{r warning = FALSE, message = FALSE}
means <- apply(minst_sample.together, 2, mean)
sds <- apply(minst_sample.together, 2, sd)
plot(means,
     sds,
     main = "Plot of Means vs. Standard Deviations",
     xlab = "Means",
     ylab = "Standard Deviations")
```
Looking at the graph, there does appear to be one distinct outlier near the bottom left, but otherwise the points all follow a very clear trend.

# Principal Components Analysis
9. Next, we would like to reduce the dimension of the data. Explain what the "dimension" of a dataset is. Discuss why one would want to reduce it. Provide an intuitive explanation of how PCA can achieve this.

The dimension of a dataset is the number of variables it is tracking. In our example, we have 784 pixels of which we are tracking the darkness. This is a lot of variables to keep track of, and may often cause unnecessary expenses or time-wasting in collecting and analyzing so many different data components. As such, it is often desirable to reeduce the data into a more manageable number of components in order to save time, particularly if the dataset can be reduced without losing much information conveyed in the data. If the dimension can be reduced to a few, then visualization of the data in a 2D plot can carry more information, since the other dimensions would be captured adequately by the few dimension that could be plotted. Furthermore, as the dimension increases, some distance measures between points converges, so that data points become more uniformally distributed. It would result in some clustering and classification method to lose sensitivity. Finally, some methods such as LDA requires the dimension to be less than the number of sample, so that the dimension must be reduced for it to be applied. 

PCA can attempt to reduce the dimension of the data by essentially reframing it in a different way. The data is transformed from its original high dimensional coordinates with many different axis to a different coordinate system. Each axis in the new coordinate system represent a different perspective of looking at the data. To reduce the dimensions, the number of axis can be minimized so that adding a new axis or perspective doesn't really change the view very much, or that the lower dimensional view with less axis looks almost as diverse as the original high dimensional view. When such suitable new coordinate is found, it retains a good portion of the variation in the dataset, but with a different orientation, while the dimension may be much lower than the original.

For instance, with our dataset, it would be highly beneficial if we could reduce the 784 pixels we track to less than 100, or even fewer, and still be able to clearly distinguish the value of the hand-drawn number. In our example, we probably can greatly reduce the dimension, since the most common data value is 0. Intuitively, the handwriting only takes up a minority of the canvas, so that PCA should be able to retain the information in the actual handwriting, while collapsing the white space.

10. Run PCA on the dataset. Decide whether to set *scale = TRUE* or *scale = FALSE* and explain your choice. Provide a numerical summary of the first 5 PCs. Note that you should leave any categorical variables out of this analysis.

The principle components were calculated without scaling the values, since all pixels are on the same scale and range, and all numerical values indicate greyscale between 0 and 255.

```{r warning = FALSE, message = FALSE}
minst_sample.together.numerical <- minst_sample.together[,-c(1)]
minst_pcs <- prcomp(minst_sample.together.numerical, scale = FALSE)
minst_pcs_sum <- summary(minst_pcs)
print(as.data.frame(minst_pcs_sum$importance[, 1:5]))
```

11. Plot a screeplot of the PCs. How many principal components are required to explain at least 80% of the variation in the data? Based on this and the plot, does it seem like PCA has done a good job in reducing the dimensionality of the data?
```{r warning = FALSE, message = FALSE}
screeplot(minst_pcs,
          npcs = 50,
          type = "lines",
          main = "Variance explained by PC")
cum_pcs_filtered <-
  minst_pcs_sum$importance[3, abs(minst_pcs_sum$importance[3,] - 0.8) < 0.01]
cum_pcs_filtered <- as.data.frame(as.list(cum_pcs_filtered))
rownames(cum_pcs_filtered) <- c("Cum. Variance Explained")
print(cum_pcs_filtered)
```

From the screeplot and principal components, PCA has clearly done a great job reducing the dimensionality of the data, as we capture 80% of the information contained by the data in just 43 variables from the original 784 pixels. Furthermore, with just the first 7 PCs, more than 50% of the variations of the data is already captured.

12. Visualize the distributions of the first 3 PCs in separate plots and comment. Are there any apparent clusters? Do any of the first 3 PCs appear most helpful for separating the data?
```{r warning = FALSE, message = FALSE}
minst_pcs_df.1 <-
  data.frame(PC = rep("PC1", length(minst_pcs$x[, 1])),
             Value = minst_pcs$x[, 1])
minst_pcs_df.1 <-
  cbind(minst_sample.together["digit"], minst_pcs_df.1)
minst_pcs_df.1[, "digit"] <- factor(minst_pcs_df.1[, "digit"])

minst_pcs_df.2 <-
  data.frame(PC = rep("PC2", length(minst_pcs$x[, 2])),
             Value = minst_pcs$x[, 2])
minst_pcs_df.2 <-
  cbind(minst_sample.together["digit"], minst_pcs_df.2)
minst_pcs_df.2[, "digit"] <- factor(minst_pcs_df.2[, "digit"])

minst_pcs_df.3 <-
  data.frame(PC = rep("PC3", length(minst_pcs$x[, 3])),
             Value = minst_pcs$x[, 3])
minst_pcs_df.3 <-
  cbind(minst_sample.together["digit"], minst_pcs_df.3)
minst_pcs_df.3[, "digit"] <- factor(minst_pcs_df.3[, "digit"])

minst_pcs_df <-
  rbind(minst_pcs_df.1, minst_pcs_df.2, minst_pcs_df.3)

ggplot(data = minst_pcs_df, aes(fill = digit)) + aes(x = Value) + geom_boxplot() +
  xlab("PC Value") + facet_grid(. ~ PC) + ggtitle("PC Score Distributions")

```
There are no clear cut lines that can cleanly separate the data points, disregarding their digit, from each other. Each bar graph overlaps with each other for all 3 PCs. However, when taking the digit into account, there are some separation either between groups of digits or single digits.

Looking at the graph of PC1, it has a separation of the digit '0' and '1' from the rest of the digit. The digit '0' tends to have PC1 values around 700 to 1300, while the digit '1' have PC1 values around or slightly below -1000. The rest of the digits are in a mix between the values of the digit '0' and '1.'

There are no clear clustering digit-wise in the graph of PC2 in comparison to PC1. However, it was able to separate the group of digits 9, 7, and 4, which all has similar PC score, from the rest of the digits.

The plot of PC3 shows less pattern than PC1 or PC2. However, it can still be distinguished that there is a band of digit 1 around the PC3 value of 0. The remaining digits are more more scattered.

13. Plot the first 3 PCs against one another (see the Biplots section of Computing Assignment 3). Are there any apparent clusters?
```{r warning = FALSE, message = FALSE}
minst_pcs_pair_df <- minst_pcs_df[minst_pcs_df$PC == "PC1", c(1, 3)]
colnames(minst_pcs_pair_df)[2] <- "PC1"
minst_pcs_pair_df <-
  cbind(minst_pcs_pair_df, minst_pcs_df[minst_pcs_df$PC == "PC2", c(3)])
colnames(minst_pcs_pair_df)[3] <- "PC2"
minst_pcs_pair_df <-
  cbind(minst_pcs_pair_df, minst_pcs_df[minst_pcs_df$PC == "PC3", c(3)])
colnames(minst_pcs_pair_df)[4] <- "PC3"
ggpairs(
  data = minst_pcs_pair_df,
  title = "Pairwise PC Plot",
  columns = 2:4,
  upper = list(continuous = "points"),
  mapping = ggplot2::aes(color = digit),
  legend = 1
)
```
Since the first 2 PC only explains about 16-17% of the variations, it is no surprise that the data blends together on the plot. There are no clear-cut spaces between sets of points located in separate regions of the plot. However, a few notable patterns still emerge.

When plotting PC2 vs PC1 against each other, with the lower PCs on the X axis, it seems that digit 0 and digits 4, 7 (which overlaps with 0 on both PC1 and PC2), are grouped together in the upper left corner of the plot. Whereas PC1 fails to distinguish digits 6, 8, 3, and 2 from each other, they are more separated in the combined plot, shifting to the bottom and bottom right from the top left corner. Finally, digit 1 is highly concentrated in the bottom left corner, but with overlaps from the other digits. Digit 0 is more concentrated at the right most part of the plot, but it still overlaps with 6 and 3.

Plotting PC3 vs PC1, the addition of PC3 didn't really contribute much to the distinctions offered by PC1, since there are no vertical displacement between the digits, and the digits are just shifted on the X(PC1) axis.

Comparing PC3 vs PC2, PC3 also did not contribute much to the separation of the digits, since the different digits are still mostly shifted on the X axis. 

# Clustering
14. Next we would like to perform a cluster analysis on the data. Explain intuitively what that means. What exactly is a cluster? What are the objects being clustered? Why might this be helpful for this dataset?

Cluster analysis aims to divides the data into different subgroups based on their attributes. It's a type of unsupervised learning to find structure in unlabeled data. For our data, the images in the same cluster should be close together, and the images in different clusters should be far apart. We aim to categorize the images into a corresponding digits from 0-9. The clustering analysis have the potential to match each image into its most likely cluster to assign a digit to the image. We can try different number of clusters for our data and observe whether the clustering method can help label the images to correct digits. 

15. Apply one of the clustering algorithms you learned about in class to your data. Depending on the algorithm you use, you may need to specify a cutpoint to obtain a single cluster label for each sample. Note also that depending on the dimension of your dataset, it may be preferable to apply the clustering algorithm to the first few PCs of your data rather than the data itself. Elaborate upon your choice.

We use K-means algorithm for clustering. From our previous PCA analysis, we noticed that 80% of the predictive information contained within the data can be captured by first 43 PCs, so we decided to use the first 43 PCs for analysis instead of the total 784 pixels. In either case, full visualization of the clusters would be difficult. However, by using 43 dimensions, the sensitivity of the Euclidean distance measurement between points would be higher than the full 784 dimensions. Since there are 10 digits, we chose k to be 10 in order to see if there are any relation between the proximity of the data points, and the digit label. 

```{r warning = FALSE, message = FALSE}
k <- 10
minst_kmeans_df_43 <- minst_pcs$x[, 1:43]
k_cluster_43 <-
  kmeans(
    minst_kmeans_df_43,
    centers = k,
    nstart = 25,
    iter.max = 1000
  )
# plots to compare
fviz_43 <-
  fviz_cluster(k_cluster_43, geom = "point", data = minst_kmeans_df_43) + ggtitle("KMeans Clusters 43 PCs")

print(fviz_43)
```

16. What did the clustering algorithm find? Are the clusters relatively homogeneous or heterogeneous? Summarize the results.

```{r warning = FALSE, message = FALSE}
# Summarize results of clustering.
minst_pcs_cluster_df.43 <-
  cbind(minst_sample.together[, c(1)], k_cluster_43$cluster)
minst_pcs_cluster_df.43 <- data.frame(minst_pcs_cluster_df.43)
minst_pcs_cluster_df.43[, 1] <- factor(minst_pcs_cluster_df.43[, 1])
minst_pcs_cluster_df.43[, 2] <- factor(minst_pcs_cluster_df.43[, 2])
colnames(minst_pcs_cluster_df.43)[1] <- "digit"
colnames(minst_pcs_cluster_df.43)[2] <- "cluster"

cluster_bar_43 <-
  ggplot(data = minst_pcs_cluster_df.43, aes(fill = digit)) + aes(x = cluster) + geom_bar() + xlab("Cluster") + ylab("Count") + ggtitle("Clusters with 43 PCs")
print(cluster_bar_43)
```
Since the dimensionality of the clusters is still pretty high, it is difficult to fully visualize. When projecting to 2 dimensions, it is no surprise that the clusters blend together, since 2 dimension is inadequate at capturing the full information contained in all 43 dimensions. However, in regards to the digits, the clusters do reveal some patterns in composition.

The clusters 2, 3, 6, and 9 are fairly homogeneous in terms of digit composition. They are made mostly of a single digit label with a few points from the other digits. The clusters 1, 5, 7, and 10 have a tripartite composition containing 3 primary digits and a few numbers of other digits. The clusters 4 and 8 have a single digit '1' which dominates its composition. However, approximately half of these clusters are made of an assortment of other digits in almost equal frequency.

From the composition of the clusters, it seems that the data points of digit 0, 2, and 6 are close to other data points of the same type, so that the clustering algorithm placed them into their own clusters. However, the digit 0 is still spread out enough in the subspace of the 43 PCs, so that the clustering algorithm put them into two different clusters. Portions of the data points for 9, 7, and 4 are closer to each other in subspace than their own category, so that they exists in two different tripartite clusters 1 and 5. The digits 3, 5, and 8 exhibit similar arrangements. The data points that represent digit 1 are also grouped near each other locally, for they dominates the composition of cluster 4 and 8. However, the points for digit 1 are also blended together with an assortment of other digits, so that the clusters for it are not as homogeneous as the clusters for the digit 0. 

17. What is the within-cluster sum of squares for the clusters you found? How does this compare to the total sum of squares?

```{r warning = FALSE, message = FALSE}
# Compute sums of squares.
withinclustersos <- data.frame(k_cluster_43$withinss)
for (i in 1:k) {
  row.names(withinclustersos)[i] <- paste0("Cluster", i)
}

compared_to_total <-
  withinclustersos[, 1] / k_cluster_43$tot.withinss
withinclustersos <- cbind(withinclustersos, compared_to_total)
withinclustersos <-
  rbind(withinclustersos, c(k_cluster_43$tot.withinss, 1))
rownames(withinclustersos)[k + 1] <- "Total"
colnames(withinclustersos) <- c("Within.Cluster", "Compared.Total")
print(withinclustersos[order(withinclustersos$Compared.Total),])
```

The within-cluster sum of squares of all clusters, and their ratio to the total sum of squares are shown in the table. We observed that the clusters 6, 8, 9, 4 have low sum of squares. The clusters 3 and 2 have medium sum of squares, and the clusters 1, 5, 7, and 10 have high sum of squares.

The clusters 6, 8, 9, 4 are all dominated by a single type of digit, as shown in the previous part. The low within cluster sum of squares confirms that the data for these digits are indeed close to each other. It also confirms the similarity of the digit '1' to the more complicated digits. The clusters 3 and 2 are also dominated by a single digit type. However, evidently, there is more diversity in the style of these digits so that they have a higher within cluster sum of squared. The tripartite clusters 1, 5, 7, and 10 have the highest within cluster sum of squares. This is not surprising, since the composition is less homogeneous. Furthermore, it shows that the digits composing these clusters have great variations in respect other points of the same digits. Furthermore, there may also be a separation between the digits, but not as significant as the rest of the digits so that the tripartite clusters are formed. 

18. Plot the first 3 PCs against one another again but include the cluster label for each point. Do any patterns emerge? Comment on what you see.

```{r warning = FALSE, message = FALSE}
# Plot PCs with clusters.
minst_pcs_pair_df <- minst_pcs_df[minst_pcs_df$PC == "PC1", c(1, 3)]
colnames(minst_pcs_pair_df)[2] <- "PC1"
minst_pcs_pair_df <-
  cbind(minst_pcs_pair_df, minst_pcs_df[minst_pcs_df$PC == "PC2", c(3)])
colnames(minst_pcs_pair_df)[3] <- "PC2"
minst_pcs_pair_df <-
  cbind(minst_pcs_pair_df, minst_pcs_df[minst_pcs_df$PC == "PC3", c(3)])
colnames(minst_pcs_pair_df)[4] <- "PC3"
minst_pcs_cluster_pair_df <-
  cbind(minst_pcs_pair_df, factor(k_cluster_43$cluster))
colnames(minst_pcs_cluster_pair_df)[5] <- "cluster"
ggpairs(
  data = minst_pcs_cluster_pair_df,
  title = "Pairwise PC Plot with Cluster",
  columns = 2:4,
  upper = list(continuous = "points"),
  mapping = ggplot2::aes(color = cluster),
  legend = 1
)

```
Similar to the pairwise PCA plot from the previous section, the data points largely blends together in a 2-dimensional plot. However, with respect to the clusters, some pattern still emerges.

From the PC2 vs PC1 plot, it appears that the cluster 4 and 1 distinctly occupies a corner of the plot, where the rest of the data points blend together.

In the PC3 vs PC1 plot, the cluster 1 is again by itself in the right of the graph. However, the cluster 4 is no longer as separate.

In the PC3 vs PC2 plot, a few clusters appear to be distinctly separated. The cluster 6 clearly occupies the lower left corner, while the cluster 8 occupies the lower right corner. The cluster 4 is somewhat separated in the upper right corner.

In all the cases, the separation is not very clear-cut, and this may be due to the limitation of the 2D plot in depicting high dimensional data.

# Classification
19. Next we would like to perform classification on the data. Explain intuitively what that means. What does it mean to classify the data? What are the objects being classified? Why might this be of interest for this dataset?

Classification is a type of supervised learning. We use labeled data in the training dataset to make predictions on unlabeled testing dataset. We have certain existing categories in our labeled data, and we are using the attributes of labeled data to build model to predict which category does the unlabeled data belongs to. In our dataset, we have labeled images in the minst_final.train, and each of the images are already matched with one of the 10 digits from 0 to 9. We wish to use our training data to classify the images in our testing data to corresponding digits from 0 to 9. 

20. Identify a categorical variable in the data to correspond to the class of each sample in the dataset. In particular, is there a variable that might be interesting to predict from the other variables?

For our dataset, we have one categorical variable, which is the labeled digit of the images (from 0-9). The digit variable might be possible to be predicted from the other pixel variables of the image. 

21. Construct a table that describes how many samples in each class fall into each cluster. How well was your previous cluster analysis able to identify the classes you chose? Are any of the clusters comprised mostly by one class?

```{r warning = FALSE, message = FALSE}
# Construct table.
cluster_label_table <- data.frame()
for (i in 1:k) {
  for (j in 1:k) {
    cluster_label_table[i, j] <-
      nrow(minst_pcs_cluster_df.43[minst_pcs_cluster_df.43$cluster == i &
                                     minst_pcs_cluster_df.43$digit == (j - 1), ])
  }
  colnames(cluster_label_table)[i] <- paste0("Digit", i - 1)
}
cluster_total <- data.frame()
for (i in 1:k) {
  cluster_total[i, 1] <-
    nrow(minst_pcs_cluster_df.43[minst_pcs_cluster_df.43$cluster == i, ])
}
cluster_label_table <-  cbind(cluster_total, cluster_label_table)
colnames(cluster_label_table)[1] <- "Total"
for (i in 1:k) {
  row.names(cluster_label_table)[i] <- paste0("Cluster", i)
}
cluster_label_table
```
The table quantitatively confirms our finding in the clustering section. The clusters 2, 3, 6, and 9 are dominated by digit 0, 6, 2, and 0 respectively with a few numbers of other digits. The clusters 4 and 8 are primarily composed of digit 1, but with a good portion of the other digit types. The clusters 1 and 5 are composed of primarily digits 4, 7, and 9. The clusters 7 and 10 are dominated by 3, 5, and 8. 

22. Randomly break your dataset into a training set (roughly 80% of the data) and a test set (roughly 20% of the data). What is the point of doing this before doing classification?

We broke our dataset of 5000 obserbations in to 4000 images of training set and 1000 images of test set. We will need to utilize the training set to train and refine our model for classification, and the test set is used to examine the effectiveness and accuracy of our prediction model on new data. By using a test set, trained model can be tested to see if it is truly universal, and to ensure that it was not over-fitted and tied down too tightly to the training data.

We also investigated whether using the principal components has any effect on the effectiveness of classification given the high dimensionality of the original data. Since 43 PCs captures about 80% of the variation in the data, we decided to also project both the training and test dataset to the first 43 PCs calculated from the training set.

```{r warning = FALSE, message = FALSE}
# Break into train and test sets.
minst_classification <- minst_sample.together ##load data

#Generate a random number that is 80% of the total number of rows in dataset.
ran <-
  sample(1:nrow(minst_classification),
         0.8 * nrow(minst_classification))

#extract training set
pcs_classify <- 43
minst_classification_train <- minst_classification[ran,-c(1)]
minst_classification_train_pcs <- prcomp(minst_classification_train)

#extract testing set
minst_classification_test <- minst_classification[-ran,-c(1)]
minst_classification_test_pcs <-
  predict(minst_classification_train_pcs, newdata = minst_classification_test)
minst_classification_test_pcs <-
  as.data.frame(minst_classification_test_pcs)
```

23. Using the class labels based on the categorical variable you chose earlier, apply a classification method to the training set. Be sure to specify any free parameters that were chosen. What is the accuracy of the fitted model on the training set?

We use KNN Classification method with k values 5 on the training set.

```{r warning = FALSE, message = FALSE}
# Run classification algorithm.
# Compute training set accuracy.

# extract labels for training
minst_target_category <- minst_classification[ran, 1]
# extract test labels to test accuracy
minst_test_category <- minst_classification[-ran, 1]

minst_train_predict.raw <-
  knn(
    minst_classification_train,
    minst_classification_train,
    cl = minst_target_category,
    k = 5
  )
minst_train_predict.pcs <-
  knn(
    minst_classification_train_pcs$x[, 1:pcs_classify],
    minst_classification_train_pcs$x[, 1:pcs_classify],
    cl = minst_target_category,
    k = 5
  )

# compute accuracy on training set
tab_train.pcs <-
  table(minst_train_predict.pcs, minst_target_category)
tab_train.raw <-
  table(minst_train_predict.raw, minst_target_category)
accuracy <- function(x) {
  sum(diag(x) / (sum(rowSums(x)))) * 100
}

train_result <- data.frame(Raw = accuracy(tab_train.raw),
                           PC43 = accuracy(tab_train.pcs))
rownames(train_result) <- c("Accuracy")
print(train_result)
```
We found that the accuracy of both the models on the training set to be between 95-96%, with the model on the first 43 PCs slightly more accurate than the model on raw data.

24. What is the accuracy of the fitted model on the test set? How does this compare to the accuracy on the training set? Does this make sense? Explain why or why not.


```{r warning = FALSE, message = FALSE}
# run knn, and we store our prediction value in minst_test_predict
minst_test_predict.pcs <-
  knn(
    minst_classification_train_pcs$x[, 1:pcs_classify],
    minst_classification_test_pcs[, 1:pcs_classify],
    cl = minst_target_category,
    k = 5
  )
minst_test_predict.raw <-
  knn(
    minst_classification_train,
    minst_classification_test,
    cl = minst_target_category,
    k = 5
  )
# Compute accuracy on test set.
test_tab.pcs <- table(minst_test_predict.pcs, minst_test_category)
test_tab.raw <- table(minst_test_predict.raw, minst_test_category)
accuracy <- function(x) {
  sum(diag(x) / (sum(rowSums(x)))) * 100
}

test_result <- data.frame(Raw = accuracy(test_tab.raw),
                          PC43 = accuracy(test_tab.pcs))

rownames(test_result) <- c("Accuracy")
print(test_result)
```
We the run the fitted model on the test set, and the accuracy of KNN on the raw test set dropped to 93.1%. The accuracy of the model on the test set projected to the first 43 PCs calculated on the training set almost didn't change, and dropped to 95.4%.

The accuracy is a bit lower compared to the training set, but the it is still quite high. Since the rule of our model is based on the proximity to the data points in the training set itself, it does make intuitive sense that the accuracy would be higher on the training set. However, with the new data from the test set, it expected that it can contain data points outside of the known neighborhood of the correct labels in the training set. Thus, there would be a greater error.

It is interesting that the dimension reduced data had a lower shrinkage in accuracy. Perhaps, it can be attribute to the removal of noice, since 43 PCs only capture about 80% of the variations.

Regardless, it is surprising that both performed pretty well given that there is a high number of dimensions in both cases.

25. Construct a confusion matrix based on the results of classification. Were there any classes on which your algorithm performed better than others? What about worse than others? Does this make sense based on the nature of the data?
```{r warning = FALSE, message = FALSE}
cm.pcs <-
  confusionMatrix(
    factor(minst_test_predict.pcs),
    factor(minst_test_category),
    dnn = c("Prediction", "Label")
  )
cm.raw <-
  confusionMatrix(
    factor(minst_test_predict.raw),
    factor(minst_test_category),
    dnn = c("Prediction", "Label")
  )

confusion.raw <-
  ggplot(as.data.frame(cm.raw$table),
         aes(Prediction, sort(Label, decreasing = T), fill = Freq)) + 
  geom_tile() + geom_text(aes(label = Freq)) + 
  scale_fill_gradient(low = "white", high = "#009194") + 
  labs(x = "Label", y = "Prediction") + scale_x_discrete(labels = paste0("D", 0:9)) + 
  scale_y_discrete(labels = paste0("D", 0:9)) + ggtitle("KNN Confusion Raw")

confusion.raw <-
  ggplot(as.data.frame(cm.pcs$table),
         aes(Prediction, sort(Label, decreasing = T), fill = Freq)) + 
  geom_tile() + geom_text(aes(label = Freq)) + 
  scale_fill_gradient(low = "white", high = "#009194") + 
  labs(x = "Label", y = "Prediction") + scale_x_discrete(labels = paste0("D", 0:9)) + 
  scale_y_discrete(labels = paste0("D", 0:9)) + ggtitle("KNN Confusion PC43")

gridExtra::grid.arrange(confusion.raw, confuion.pcs, ncol = 2)

```
According to the matrices, the KNN on the raw data and the projected data both performed decently on all of the digits. They also make the same kind of mis-classification, but the model on the 43 PCs are slightly more sensitive in some cases.

In both cases, there are some minor mislabeling for digits such as the digit 0. The significant errors are the mis-labeling of digit 1 as 2 in both models, digit 3 as 1 and 4 in the raw model, digit 6 as 1 in raw, and digit 9 as 2 and 5 in both.

The confusion of 1 as 2 makes sense because 1 and 2 both have a central stroke downward, and are both fairly simple. Thus, some of 1s may be in the vicinity of 2s depending on the style of the writer. The confusion of 3 as 1 and 4 is less intuitive, and an examination of the few cases of actual image could shed more light on it. The digit 6 as 1 in the raw KNN is also not as intuitive, but perhaps the 6 is written such that the vertical stroke is longer and the loop is less significant. In this case, the PC projected subspace seem to be able to separate them better. The confusion of 9 as 2 and 5 would make sense if the mis-classified cases does not close off the circular portion of 9 very well, and it would make the 9 looks similar to 2 or 5.

Both the raw and PC projected KNN did really well on labeling the digit 2 as 2. This may be due to the fact that the digit '2' does not have any loops or turns that would cause it to look similar to the other digits if not written with care.

In general, it seems that in some cases the PC projected KNN offers more separation between the digits, and sometimes it reduces the separation. However, overall, it helps to distinguish some of the digits slightly more than KNN on just the raw data. This may be due to the fact that the Euclidean distance loses sensitivity as the dimensions increases. Furthermore, with only 80% of the variations, the projection to the 43 PCs may reduce the noise in the data. However, by using the 43 PCs, about 20% of the variations are not captured, and this may lead to greater confusion in some cases.

# Conclusion
From our inspection and analysis of the subset of the MNIST handwriting dataset, we are able to confirm some initial intuitions about the data, and eventually generate a classification rule that can label the data with as much as 95.4% accuracy on the testing data set. Intuitively, the images in the MNIST dataset should be mostly made of blank spaces. This presupposition is confirmed by our initial analysis of the pixels, showing that the most common pixel value is indeed 0. The mean of all the pixels is at 33.47, with a standard deviation of 78.7, showing that the center of the data leans towards 0. The maximum pixel value is the darkest value of 255, indicating that for handwriting, the intensity of stroke may be fully dark. From our correlation analysis between two adjacent pixels, it suggests that adjacent pixels may have high correlation, so that they are probably not independent from each other. This also confirms the intuitation that the strokes are connected, and where the stroke is going depends on the previous portion of the writing. We are able to reduce the dimension of the data using PCA from 784 dimensions to 43 dimensions while retaining 80% of the variations. Furthermore, the variations captured by the PCs increase quickly, with the first 7 PCs capturing more than 50% of the variations. However, the dimensionality is still high enough, so that visualization of the PCs in 2-D plot is not really adequate in clearly separating the data. We used the KMeans clustering method to find 10 clusters in the dataset projected to the first 43 PCs. We did find that the images of the same digit indeed are close to each other. However, in some cases, such as the digits 4, 7 and 9, the data points are more or less blended together, so that the separation is not very clear. Furthermore, the digits seem to span across a wide range of values, so that two clusters composed primarily of one digit may be formed. We also applied the KNN classification method to the dataset with a k value of 5. We experimented with classification using either the full dataset, or the dataset projected to the first 43 PCs. We found that KNN performs decently on the dataset despite the high dimension in both cases, with a training accruacy of 95-96%, and a shrinkage of only 2% or almost none. In general, the model using dataset projected to the first 43 PCs performed better than the raw model. This suggests that PCA for the MNIST dataset may reduce noise, and also increase the sensitivity of the proximity measurement with lower dimensions than the full 784 pixels. The KNN classification performs decently on all of the digit for both the raw and projected model. In the few cases where there are more mistakes than usual, the errors match the intuition regarding the stylistic choices of writing the digit for the most part. Therefore, it shows that KNN, without assuming anything regarding the distribution or relation of the data, performs well on the MNIST dataset. Hence, it would be interesting to investigate whether by using more information regarding the distribution of the data, or the relationship between proximate pixels, an even greater performance could be achieved. Such analysis provides convincing evidence for the ability of machine learning techniques to translate human handwriting more generally into classified characters with high degrees of accuracy. It also provide a solid foundation toward automating the computerization of handwritten data from medical records, postal services, or any number of other sources. 

